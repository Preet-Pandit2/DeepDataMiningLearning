{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d498b95",
   "metadata": {},
   "source": [
    "# üîó Repository Information\n",
    "\n",
    "**This notebook uses a FORKED repository**\n",
    "\n",
    "- **Original Repository**: https://github.com/lkk688/DeepDataMiningLearning\n",
    "- **Fork**: https://github.com/Preet-Pandit2/DeepDataMiningLearning\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa0eb7",
   "metadata": {},
   "source": [
    "# CMPE 249 Homework: CustomRCNN Training on Waymo Dataset\n",
    "\n",
    "## Assignment Overview\n",
    "- **Task**: Modify and train a CustomRCNN (Faster R-CNN) model\n",
    "- **Dataset**: Waymo Open Dataset (COCO format, 10000 images)\n",
    "- **Modifications**: Enhanced FPN with attention mechanism and improved detection head\n",
    "- **Objective**: Compare original vs modified model performance using mAP metrics\n",
    "\n",
    "## Author: Preet Kamalnayan Pandit\n",
    "## Student ID: 018171543\n",
    "## Date: October 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58757c5",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ece8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    # Mount Google Drive for data storage\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cede604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q pycocotools\n",
    "!pip install -q torchinfo\n",
    "!pip install -q matplotlib seaborn\n",
    "!pip install -q tqdm\n",
    "!pip install -q opencv-python\n",
    "\n",
    "print(\"‚úÖ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5872af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/Preet-Pandit2/DeepDataMiningLearning.git\n",
    "%cd DeepDataMiningLearning\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.append('/content/DeepDataMiningLearning')\n",
    "\n",
    "print(f\"‚úÖ Repository cloned from: https://github.com/{YOUR_GITHUB_USERNAME}/DeepDataMiningLearning.git\")\n",
    "print(\"‚úÖ Path configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5236b60",
   "metadata": {},
   "source": [
    "## 2. Download and Setup Waymo Dataset\n",
    "\n",
    "**Dataset Location**: `My Drive/waymo_coco__10000_step10/`\n",
    "\n",
    "The notebook will:\n",
    "1. Locate your `annotations.json` file\n",
    "2. Extract `images.zip` automatically\n",
    "3. Verify the dataset structure\n",
    "\n",
    "After extraction, the structure will be:\n",
    "```\n",
    "waymo_coco__10000_step10/\n",
    "‚îú‚îÄ‚îÄ annotations.json      (COCO format annotations)\n",
    "‚îú‚îÄ‚îÄ images.zip           (compressed images)\n",
    "‚îî‚îÄ‚îÄ images/              (Extracted images)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up data directory - using your Google Drive path\n",
    "if IN_COLAB:\n",
    "    # Your Waymo dataset location in Google Drive\n",
    "    WAYMO_PATH = '/content/drive/MyDrive/waymo_coco__10000_step10'\n",
    "    DATA_ROOT = '/content/drive/MyDrive'\n",
    "else:\n",
    "    DATA_ROOT = './data'\n",
    "    WAYMO_PATH = os.path.join(DATA_ROOT, 'waymo_coco__10000_step10')\n",
    "\n",
    "# Output directory for results\n",
    "OUTPUT_DIR = os.path.join(DATA_ROOT, 'CMPE249_HW_Output')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data root: {DATA_ROOT}\")\n",
    "print(f\"Waymo dataset path: {WAYMO_PATH}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Check and extract Waymo dataset\n",
    "print(\"Checking Waymo dataset...\")\n",
    "\n",
    "# Paths to your files\n",
    "annotations_file = os.path.join(WAYMO_PATH, 'annotations.json')\n",
    "images_zip = os.path.join(WAYMO_PATH, 'images.zip')\n",
    "images_dir = os.path.join(WAYMO_PATH, 'images')\n",
    "\n",
    "# Check if annotations file exists\n",
    "if os.path.exists(annotations_file):\n",
    "    print(f\"‚úÖ Found annotations.json\")\n",
    "    # Get file size\n",
    "    size_mb = os.path.getsize(annotations_file) / (1024 * 1024)\n",
    "    print(f\"   Size: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå annotations.json not found at {annotations_file}\")\n",
    "    print(\"   Please check your Google Drive path!\")\n",
    "\n",
    "# Check and extract images.zip if needed\n",
    "if os.path.exists(images_zip):\n",
    "    print(f\"‚úÖ Found images.zip\")\n",
    "    size_mb = os.path.getsize(images_zip) / (1024 * 1024)\n",
    "    print(f\"   Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Extract if images directory doesn't exist\n",
    "    if not os.path.exists(images_dir):\n",
    "        print(f\"\\nüì¶ Extracting images.zip...\")\n",
    "        print(\"   This may take a few minutes...\")\n",
    "        \n",
    "        with zipfile.ZipFile(images_zip, 'r') as zip_ref:\n",
    "            zip_ref.extractall(WAYMO_PATH)\n",
    "        \n",
    "        print(f\"‚úÖ Images extracted to {images_dir}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Images directory already exists\")\n",
    "        # Count images\n",
    "        num_images = len([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        print(f\"   Found {num_images} images\")\n",
    "else:\n",
    "    print(f\"‚ùå images.zip not found at {images_zip}\")\n",
    "    print(\"   Please check your Google Drive path!\")\n",
    "\n",
    "# Verify final structure\n",
    "print(f\"\\nüìÅ Dataset structure:\")\n",
    "print(f\"{WAYMO_PATH}/\")\n",
    "if os.path.exists(annotations_file):\n",
    "    print(\"  ‚úÖ annotations.json\")\n",
    "if os.path.exists(images_zip):\n",
    "    print(\"  ‚úÖ images.zip\")\n",
    "if os.path.exists(images_dir):\n",
    "    print(\"  ‚úÖ images/\")\n",
    "    # Show subdirectories if any\n",
    "    subdirs = [d for d in os.listdir(images_dir) if os.path.isdir(os.path.join(images_dir, d))]\n",
    "    if subdirs:\n",
    "        for subdir in subdirs:\n",
    "            print(f\"      ‚îî‚îÄ‚îÄ {subdir}/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if os.path.exists(annotations_file) and os.path.exists(images_dir):\n",
    "    print(\"‚úÖ Dataset is ready for training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset setup incomplete. Please check the paths above.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb28a2d",
   "metadata": {},
   "source": [
    "## 3. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aae42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4530a14",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Modifications\n",
    "\n",
    "### üìä Comparison Overview\n",
    "\n",
    "| Aspect | Original Model | Modified Model | Improvement |\n",
    "|--------|---------------|----------------|-------------|\n",
    "| **Backbone** | ResNet50 | ResNet101 | +75% params in backbone |\n",
    "| **Trainable Layers** | 3 layers | 5 layers | +67% trainable capacity |\n",
    "| **FPN Enhancement** | Standard FPN | FPN + Attention | Channel & Spatial attention |\n",
    "| **ROI Head** | Single FC | Multi-layer FC | +2 layers + dropout |\n",
    "| **Regularization** | Basic | Enhanced | BN + Dropout |\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Detailed Modifications\n",
    "\n",
    "#### **Modification 1: Enhanced Backbone (ResNet50 ‚Üí ResNet101)**\n",
    "- **Original**: ResNet50 with 25.6M parameters\n",
    "- **Modified**: ResNet101 with 44.5M parameters\n",
    "- **Benefit**: Deeper network captures more complex features\n",
    "- **Expected improvement**: +1-2% mAP\n",
    "\n",
    "#### **Modification 2: Channel Attention Module (CAM)**\n",
    "```\n",
    "Input Feature ‚Üí Avg Pool ‚Üí MLP ‚Üí \n",
    "              ‚Üí Max Pool ‚Üí MLP ‚Üí \n",
    "              ‚Üí Sigmoid ‚Üí Multiply with Input\n",
    "```\n",
    "- **Purpose**: Identifies which feature channels are most important\n",
    "- **Reduction ratio**: 16 (balances performance and efficiency)\n",
    "- **Location**: Applied to FPN layers\n",
    "- **Benefit**: Better feature selection, focuses on discriminative channels\n",
    "- **Expected improvement**: +0.5-1% mAP\n",
    "\n",
    "#### **Modification 3: Spatial Attention Module (SAM)**\n",
    "```\n",
    "Input Feature ‚Üí Channel Pooling (Avg + Max) ‚Üí \n",
    "              ‚Üí Conv 7√ó7 ‚Üí Sigmoid ‚Üí Multiply with Input\n",
    "```\n",
    "- **Purpose**: Identifies where important features are located spatially\n",
    "- **Kernel size**: 7√ó7 for larger receptive field\n",
    "- **Location**: Applied after channel attention in FPN\n",
    "- **Benefit**: Focuses on important spatial regions\n",
    "- **Expected improvement**: +0.5-1% mAP\n",
    "\n",
    "#### **Modification 4: Enhanced FPN Layers**\n",
    "```\n",
    "Original:  Conv ‚Üí Conv\n",
    "Modified:  Conv ‚Üí Channel Attention ‚Üí Spatial Attention ‚Üí Conv ‚Üí BN ‚Üí ReLU\n",
    "```\n",
    "- **Additional parameters**: ~5% increase\n",
    "- **Benefit**: Much better multi-scale feature fusion\n",
    "- **Expected improvement**: +1-2% mAP\n",
    "\n",
    "#### **Modification 5: Improved ROI Head**\n",
    "```\n",
    "Original:  FC(in_features ‚Üí num_classes)\n",
    "Modified:  FC(in_features ‚Üí 1024) ‚Üí BN ‚Üí ReLU ‚Üí Dropout(0.5) ‚Üí\n",
    "           FC(1024 ‚Üí 512) ‚Üí BN ‚Üí ReLU ‚Üí Dropout(0.3) ‚Üí\n",
    "           FC(512 ‚Üí num_classes)\n",
    "```\n",
    "- **Hidden dimensions**: 1024 ‚Üí 512\n",
    "- **Dropout rates**: 0.5 and 0.3 for regularization\n",
    "- **Batch normalization**: After each FC layer\n",
    "- **Benefit**: Better classification capacity, prevents overfitting\n",
    "- **Expected improvement**: +1-2% mAP\n",
    "\n",
    "#### **Modification 6: Increased Trainable Layers**\n",
    "- **Original**: Last 3 layers trainable (more frozen backbone)\n",
    "- **Modified**: Last 5 layers trainable (more flexibility)\n",
    "- **Benefit**: Better adaptation to Waymo dataset\n",
    "- **Tradeoff**: Longer training time, higher memory\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Expected Overall Improvement\n",
    "- **Combined mAP improvement**: +3-7%\n",
    "- **Training time**: ~20-30% longer due to larger model\n",
    "- **Memory usage**: ~40% more GPU memory\n",
    "- **Inference speed**: Slightly slower (~10-15%)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why These Modifications?\n",
    "\n",
    "1. **Attention Mechanisms**: State-of-the-art in computer vision, proven to improve detection\n",
    "2. **Deeper Backbone**: ResNet101 is standard for high-accuracy detection\n",
    "3. **Enhanced ROI Head**: Increases model capacity for complex scene understanding\n",
    "4. **Regularization**: Prevents overfitting on training data\n",
    "5. **More Trainable Layers**: Better domain adaptation for autonomous driving\n",
    "\n",
    "These are **medium-level modifications** that significantly improve performance while maintaining reasonable training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create modified CustomRCNN model\n",
    "# We'll create a new file with our modifications\n",
    "\n",
    "modified_model_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel Attention Module for FPN enhancement\"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return x * self.sigmoid(out)\n",
    "\n",
    "class EnhancedFPNWithAttention(nn.Module):\n",
    "    \"\"\"Enhanced FPN with Channel Attention\"\"\"\n",
    "    def __init__(self, in_channels_list, out_channels):\n",
    "        super().__init__()\n",
    "        # Standard FPN layers\n",
    "        self.inner_blocks = nn.ModuleList()\n",
    "        self.layer_blocks = nn.ModuleList()\n",
    "        \n",
    "        for in_channels in in_channels_list:\n",
    "            inner_block = nn.Conv2d(in_channels, out_channels, 1)\n",
    "            layer_block = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "            self.inner_blocks.append(inner_block)\n",
    "            self.layer_blocks.append(layer_block)\n",
    "        \n",
    "        # Add channel attention modules\n",
    "        self.attention_modules = nn.ModuleList([\n",
    "            ChannelAttention(out_channels) for _ in in_channels_list\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is OrderedDict of feature maps\n",
    "        results = []\n",
    "        names = list(x.keys())\n",
    "        x_list = list(x.values())\n",
    "        \n",
    "        # Top-down pathway\n",
    "        last_inner = self.inner_blocks[-1](x_list[-1])\n",
    "        results.append(self.layer_blocks[-1](last_inner))\n",
    "        \n",
    "        for idx in range(len(x_list) - 2, -1, -1):\n",
    "            inner_lateral = self.inner_blocks[idx](x_list[idx])\n",
    "            feat_shape = inner_lateral.shape[-2:]\n",
    "            inner_top_down = F.interpolate(last_inner, size=feat_shape, mode=\"nearest\")\n",
    "            last_inner = inner_lateral + inner_top_down\n",
    "            \n",
    "            # Apply attention\n",
    "            last_inner = self.attention_modules[idx](last_inner)\n",
    "            \n",
    "            results.insert(0, self.layer_blocks[idx](last_inner))\n",
    "        \n",
    "        return {name: res for name, res in zip(names, results)}\n",
    "\n",
    "class ImprovedROIHead(nn.Module):\n",
    "    \"\"\"Improved ROI Head with additional FC layer\"\"\"\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        # Additional FC layer for better feature extraction\n",
    "        self.fc1 = nn.Linear(in_features, in_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "'''\n",
    "\n",
    "# Save the modified model code\n",
    "with open('modified_customrcnn.py', 'w') as f:\n",
    "    f.write(modified_model_code)\n",
    "\n",
    "print(\"‚úÖ Modified model architecture created!\")\n",
    "print(\"\\nModifications implemented:\")\n",
    "print(\"1. ‚úì Channel Attention Module (CAM) for FPN\")\n",
    "print(\"2. ‚úì Enhanced FPN with attention mechanism\")\n",
    "print(\"3. ‚úì Improved ROI Head with additional FC layer + dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66414467",
   "metadata": {},
   "source": [
    "### üé® Visual Architecture Comparison\n",
    "\n",
    "```\n",
    "ORIGINAL CustomRCNN:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Input Image                                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  ResNet50 Backbone   ‚îÇ  (25.6M params)\n",
    "         ‚îÇ  (3 trainable layers)‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ   Standard FPN       ‚îÇ  (No attention)\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ     RPN              ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  Simple ROI Head     ‚îÇ  (1 FC layer)\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  Detection Output    ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "\n",
    "MODIFIED CustomRCNN (OUR IMPROVEMENTS):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Input Image                                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  ResNet101 Backbone  ‚îÇ  ‚≠ê +75% params\n",
    "         ‚îÇ  (5 trainable layers)‚îÇ  ‚≠ê More flexibility\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  Enhanced FPN        ‚îÇ  ‚≠ê NEW!\n",
    "         ‚îÇ  + Channel Attention ‚îÇ  ‚≠ê Better features\n",
    "         ‚îÇ  + Spatial Attention ‚îÇ  ‚≠ê Focus mechanism\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ     RPN              ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  Enhanced ROI Head   ‚îÇ  ‚≠ê 3 FC layers\n",
    "         ‚îÇ  + Dropout           ‚îÇ  ‚≠ê Regularization\n",
    "         ‚îÇ  + Batch Norm        ‚îÇ  ‚≠ê Stability\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ  Detection Output    ‚îÇ  ‚≠ê Better accuracy\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Legend: ‚≠ê = Our modifications\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819539bc",
   "metadata": {},
   "source": [
    "## 5. Load Dataset\n",
    "\n",
    "Load the Waymo dataset in COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed011d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset utilities from the repository\n",
    "try:\n",
    "    from DeepDataMiningLearning.detection.dataset import get_dataset\n",
    "    from DeepDataMiningLearning.detection import utils\n",
    "    print(\"‚úÖ Successfully imported dataset utilities\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Import error: {e}\")\n",
    "    print(\"Please ensure the repository is properly cloned and in the Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_path = WAYMO_PATH  # Base path: waymo_coco__10000_step10\n",
    "        self.annotationfile = os.path.join(WAYMO_PATH, 'annotations.json')  # Your annotations file\n",
    "        self.dataset = 'waymococo'  # Dataset type\n",
    "        self.data_augmentation = 'hflip'  # Horizontal flip augmentation\n",
    "        self.backend = 'PIL'\n",
    "        self.use_v2 = False\n",
    "        # Batch size optimized for A100 GPU (40GB/80GB)\n",
    "        # A100: Can use batch_size=8 or even 16\n",
    "        # T4/P100: Use batch_size=4\n",
    "        self.batch_size = 8  # Optimized for A100 GPU\n",
    "        self.workers = 4  # Increase workers for faster data loading on A100\n",
    "\n",
    "args = Args()\n",
    "\n",
    "print(\"Dataset Configuration:\")\n",
    "print(f\"  üìä Optimized for A100 GPU\")\n",
    "print(f\"  Data path: {args.data_path}\")\n",
    "print(f\"  Annotations: {args.annotationfile}\")\n",
    "print(f\"  Dataset type: {args.dataset}\")\n",
    "print(f\"  Batch size: {args.batch_size}\")\n",
    "print(f\"  Workers: {args.workers}\")\n",
    "\n",
    "# Load training and validation datasets\n",
    "try:\n",
    "    print(\"\\nLoading training dataset...\")\n",
    "    train_dataset, num_classes = get_dataset(\n",
    "        args.dataset, \n",
    "        is_train=True, \n",
    "        is_val=False, \n",
    "        args=args\n",
    "    )\n",
    "    \n",
    "    print(\"Loading validation dataset...\")\n",
    "    val_dataset, _ = get_dataset(\n",
    "        args.dataset, \n",
    "        is_train=False, \n",
    "        is_val=True, \n",
    "        args=args\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datasets loaded successfully!\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading dataset: {e}\")\n",
    "    print(\"Please ensure the Waymo dataset is properly downloaded and structured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=args.workers,\n",
    "    collate_fn=utils.collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaders created!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28e9fde",
   "metadata": {},
   "source": [
    "## 6. Visualize Dataset Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c209f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "def visualize_sample(dataset, idx=None, save_path=None):\n",
    "    \"\"\"Visualize a sample from the dataset\"\"\"\n",
    "    if idx is None:\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "    \n",
    "    image, target = dataset[idx]\n",
    "    \n",
    "    # Convert to uint8 for visualization\n",
    "    if image.dtype == torch.float32:\n",
    "        image = (image * 255).to(torch.uint8)\n",
    "    \n",
    "    # Draw bounding boxes\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "    \n",
    "    # Create label strings\n",
    "    label_strs = [f\"Class {l.item()}\" for l in labels]\n",
    "    \n",
    "    # Draw boxes\n",
    "    img_with_boxes = draw_bounding_boxes(\n",
    "        image, \n",
    "        boxes, \n",
    "        labels=label_strs,\n",
    "        colors=\"red\",\n",
    "        width=2\n",
    "    )\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(to_pil_image(img_with_boxes))\n",
    "    plt.title(f\"Sample {idx}: {len(boxes)} objects\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize a few samples\n",
    "print(\"Visualizing dataset samples...\")\n",
    "for i in range(3):\n",
    "    visualize_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fecc6a9",
   "metadata": {},
   "source": [
    "## 6.5 Import Modified CustomRCNN from Repository\n",
    "\n",
    "Our architectural modifications are now in the repository file:\n",
    "`DeepDataMiningLearning/detection/modeling_customrcnn_modified.py`\n",
    "\n",
    "This file contains:\n",
    "- ‚úÖ Channel Attention Module (CAM)\n",
    "- ‚úÖ Spatial Attention Module (SAM)\n",
    "- ‚úÖ Enhanced FPN with CBAM attention\n",
    "- ‚úÖ Improved ROI Head with multi-layer FC + dropout\n",
    "- ‚úÖ ModifiedCustomRCNN class ready to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ModifiedCustomRCNN from repository\n",
    "from DeepDataMiningLearning.detection.modeling_customrcnn_modified import (\n",
    "    ModifiedCustomRCNN,\n",
    "    create_modified_customrcnn,\n",
    "    ChannelAttentionModule,\n",
    "    SpatialAttentionModule,\n",
    "    CBAM,\n",
    "    EnhancedFPNWithAttention,\n",
    "    EnhancedFastRCNNPredictor\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Successfully imported ModifiedCustomRCNN from repository!\")\n",
    "print(\"\\nAvailable components:\")\n",
    "print(\"  ‚Ä¢ ModifiedCustomRCNN - Main model class\")\n",
    "print(\"  ‚Ä¢ create_modified_customrcnn - Factory function\")\n",
    "print(\"  ‚Ä¢ ChannelAttentionModule - Channel attention\")\n",
    "print(\"  ‚Ä¢ SpatialAttentionModule - Spatial attention\")\n",
    "print(\"  ‚Ä¢ CBAM - Combined attention module\")\n",
    "print(\"  ‚Ä¢ EnhancedFPNWithAttention - Attention-enhanced FPN\")\n",
    "print(\"  ‚Ä¢ EnhancedFastRCNNPredictor - Multi-layer ROI head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36688b7",
   "metadata": {},
   "source": [
    "## 7. Create Models\n",
    "\n",
    "Create both original and modified CustomRCNN models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544309c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepDataMiningLearning.detection.models import create_detectionmodel\n",
    "\n",
    "# Create original CustomRCNN model (baseline)\n",
    "print(\"Creating ORIGINAL CustomRCNN (Baseline)...\")\n",
    "model_original, _, _ = create_detectionmodel(\n",
    "    modelname='customrcnn_resnet50',\n",
    "    num_classes=num_classes,\n",
    "    trainable_layers=3,  # Fine-tune last 3 layers\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORIGINAL Model Created Successfully!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_original.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_original.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MODIFIED CustomRCNN using our repository implementation\n",
    "print(\"Creating MODIFIED CustomRCNN with full architectural enhancements...\")\n",
    "print(\"\\nUsing ModifiedCustomRCNN from repository:\")\n",
    "print(\"  File: DeepDataMiningLearning/detection/modeling_customrcnn_modified.py\")\n",
    "\n",
    "# Create modified model with all enhancements\n",
    "model_modified = create_modified_customrcnn(\n",
    "    backbone='resnet101',           # Larger backbone (ResNet50 ‚Üí ResNet101)\n",
    "    num_classes=num_classes,        # Dataset-specific classes\n",
    "    trainable_layers=5,             # More trainable layers (3 ‚Üí 5)\n",
    "    use_attention=True,             # ‚úÖ Enable CBAM attention in FPN\n",
    "    enhanced_roi_head=True,         # ‚úÖ Enable multi-layer ROI head\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ARCHITECTURAL ENHANCEMENTS APPLIED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Backbone Enhancement:\")\n",
    "print(\"   ResNet50 (25.6M params) ‚Üí ResNet101 (44.5M params)\")\n",
    "print(\"   Impact: +75% capacity in feature extraction\")\n",
    "\n",
    "print(\"\\n2. FPN with CBAM Attention:\")\n",
    "print(\"   ‚Ä¢ Channel Attention Module (CAM)\")\n",
    "print(\"     - Identifies important feature channels\")\n",
    "print(\"     - Reduction ratio: 16\")\n",
    "print(\"   ‚Ä¢ Spatial Attention Module (SAM)\")\n",
    "print(\"     - Identifies important spatial locations\")\n",
    "print(\"     - Kernel size: 7x7\")\n",
    "print(\"   Impact: Better multi-scale feature learning\")\n",
    "\n",
    "print(\"\\n3. Enhanced ROI Head:\")\n",
    "print(\"   Original: Single FC layer\")\n",
    "print(\"   Modified: FC(1024) ‚Üí BN ‚Üí Dropout(0.5) ‚Üí FC(512) ‚Üí BN ‚Üí Dropout(0.3) ‚Üí FC(classes)\")\n",
    "print(\"   Impact: Better classification with regularization\")\n",
    "\n",
    "print(\"\\n4. Training Configuration:\")\n",
    "print(f\"   Trainable layers: 3 ‚Üí 5 (more flexibility)\")\n",
    "print(f\"   Dropout rates: 0.5 and 0.3 (prevent overfitting)\")\n",
    "print(f\"   Batch normalization: After each FC layer (training stability)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Modified model created with ALL enhancements from repository!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count parameters\n",
    "total_params_mod = sum(p.numel() for p in model_modified.parameters())\n",
    "trainable_params_mod = sum(p.numel() for p in model_modified.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original Model:  {total_params:,} total | {trainable_params:,} trainable\")\n",
    "print(f\"Modified Model:  {total_params_mod:,} total | {trainable_params_mod:,} trainable\")\n",
    "print(f\"Difference:      +{total_params_mod - total_params:,} params ({((total_params_mod - total_params) / total_params * 100):.1f}% increase)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc74f29",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "EPOCHS = 10  # 10 epochs for good convergence with A100 GPU (~7-9 hours total)\n",
    "             # This provides a good balance between training time and performance\n",
    "LEARNING_RATE = 0.005\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0005\n",
    "LR_STEP_SIZE = 5  # Reduce LR at epoch 5 (halfway through training)\n",
    "LR_GAMMA = 0.1\n",
    "\n",
    "# Output directory (already created above)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'original'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'modified'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, 'visualizations'), exist_ok=True)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  LR Step Size: {LR_STEP_SIZE} (LR reduced at epoch {LR_STEP_SIZE})\")\n",
    "print(f\"  Batch Size: {args.batch_size}\")\n",
    "print(f\"  GPU: A100 (expected)\")\n",
    "print(f\"  Estimated Time:\")\n",
    "print(f\"    - Original model: ~3-4 hours\")\n",
    "print(f\"    - Modified model: ~4-5 hours\")\n",
    "print(f\"    - Total: ~7-9 hours\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64115cec",
   "metadata": {},
   "source": [
    "## 9. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    loss_dict_sum = defaultdict(float)\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch}\")\n",
    "    for i, (images, targets) in enumerate(pbar):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v \n",
    "                    for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        total_loss = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track losses\n",
    "        losses.append(total_loss.item())\n",
    "        for k, v in loss_dict.items():\n",
    "            loss_dict_sum[k] += v.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        if i % print_freq == 0:\n",
    "            avg_loss = np.mean(losses[-print_freq:])\n",
    "            pbar.set_postfix({'loss': f'{avg_loss:.4f}'})\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_losses = {k: v / len(data_loader) for k, v in loss_dict_sum.items()}\n",
    "    avg_total_loss = np.mean(losses)\n",
    "    \n",
    "    return avg_total_loss, avg_losses\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs, output_dir, model_name=\"model\"):\n",
    "    \"\"\"Complete training pipeline\"\"\"\n",
    "    # Optimizer and scheduler\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(\n",
    "        params,\n",
    "        lr=LEARNING_RATE,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=LR_STEP_SIZE,\n",
    "        gamma=LR_GAMMA\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting Training: {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_loss_dict = train_one_epoch(\n",
    "            model, optimizer, train_loader, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch}/{epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            checkpoint_path = os.path.join(output_dir, f'{model_name}_best.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"  ‚úÖ Best model saved! (loss: {best_loss:.4f})\")\n",
    "        \n",
    "        # Save regular checkpoint every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            checkpoint_path = os.path.join(output_dir, f'{model_name}_epoch{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "            }, checkpoint_path)\n",
    "        \n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = os.path.join(output_dir, f'{model_name}_history.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"‚úÖ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359efce",
   "metadata": {},
   "source": [
    "## 10. Train Original Model (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67029c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train original model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ORIGINAL MODEL (BASELINE)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history_original = train_model(\n",
    "    model_original,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    output_dir=os.path.join(OUTPUT_DIR, 'original'),\n",
    "    model_name='original_customrcnn'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Original model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe0dbb",
   "metadata": {},
   "source": [
    "## 11. Train Modified Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train modified model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING MODIFIED MODEL\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history_modified = train_model(\n",
    "    model_modified,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    output_dir=os.path.join(OUTPUT_DIR, 'modified'),\n",
    "    model_name='modified_customrcnn'\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Modified model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4d848",
   "metadata": {},
   "source": [
    "## 12. Evaluation with mAP Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ba5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepDataMiningLearning.detection.myevaluator import simplemodelevaluate\n",
    "\n",
    "def evaluate_model(model, data_loader, model_name=\"model\"):\n",
    "    \"\"\"Evaluate model and return mAP metrics\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Use the evaluation function from the repository\n",
    "    results = simplemodelevaluate(model, data_loader, device)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoints for evaluation\n",
    "print(\"Loading best model checkpoints...\")\n",
    "\n",
    "# Load original model\n",
    "checkpoint_original = torch.load(\n",
    "    os.path.join(OUTPUT_DIR, 'original', 'original_customrcnn_best.pth')\n",
    ")\n",
    "model_original.load_state_dict(checkpoint_original['model_state_dict'])\n",
    "print(f\"‚úÖ Original model loaded (Epoch {checkpoint_original['epoch']})\")\n",
    "\n",
    "# Load modified model\n",
    "checkpoint_modified = torch.load(\n",
    "    os.path.join(OUTPUT_DIR, 'modified', 'modified_customrcnn_best.pth')\n",
    ")\n",
    "model_modified.load_state_dict(checkpoint_modified['model_state_dict'])\n",
    "print(f\"‚úÖ Modified model loaded (Epoch {checkpoint_modified['epoch']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cdcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "print(\"\\nEvaluating models on validation set...\\n\")\n",
    "\n",
    "results_original = evaluate_model(\n",
    "    model_original, \n",
    "    val_loader, \n",
    "    model_name=\"Original CustomRCNN\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "results_modified = evaluate_model(\n",
    "    model_modified, \n",
    "    val_loader, \n",
    "    model_name=\"Modified CustomRCNN\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b81448",
   "metadata": {},
   "source": [
    "## 13. Results Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_original['train_loss'], label='Original Model', linewidth=2)\n",
    "plt.plot(history_modified['train_loss'], label='Modified Model', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Training Loss', fontsize=12)\n",
    "plt.title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_original['learning_rate'], label='Learning Rate', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training curves saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Total Parameters',\n",
    "        'Trainable Parameters',\n",
    "        'Final Training Loss',\n",
    "        'Best Training Loss',\n",
    "        'mAP@0.5',\n",
    "        'mAP@0.5:0.95',\n",
    "    ],\n",
    "    'Original Model': [\n",
    "        f\"{total_params:,}\",\n",
    "        f\"{trainable_params:,}\",\n",
    "        f\"{history_original['train_loss'][-1]:.4f}\",\n",
    "        f\"{min(history_original['train_loss']):.4f}\",\n",
    "        \"TBD\",  # Fill with actual results\n",
    "        \"TBD\",\n",
    "    ],\n",
    "    'Modified Model': [\n",
    "        f\"{total_params_mod:,}\",\n",
    "        f\"{trainable_params_mod:,}\",\n",
    "        f\"{history_modified['train_loss'][-1]:.4f}\",\n",
    "        f\"{min(history_modified['train_loss']):.4f}\",\n",
    "        \"TBD\",\n",
    "        \"TBD\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Save comparison table\n",
    "df_comparison.to_csv(os.path.join(OUTPUT_DIR, 'comparison_results.csv'), index=False)\n",
    "print(\"‚úÖ Comparison results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a1b958",
   "metadata": {},
   "source": [
    "## 14. Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8707e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, num_samples=5, conf_threshold=0.5):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        image, target = dataset[idx]\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model([image.to(device)])[0]\n",
    "        \n",
    "        # Filter by confidence\n",
    "        keep = prediction['scores'] > conf_threshold\n",
    "        boxes = prediction['boxes'][keep].cpu()\n",
    "        labels = prediction['labels'][keep].cpu()\n",
    "        scores = prediction['scores'][keep].cpu()\n",
    "        \n",
    "        # Convert image for visualization\n",
    "        if image.dtype == torch.float32:\n",
    "            img_vis = (image * 255).to(torch.uint8)\n",
    "        else:\n",
    "            img_vis = image\n",
    "        \n",
    "        # Create labels with scores\n",
    "        label_strs = [f\"C{l.item()}: {s:.2f}\" for l, s in zip(labels, scores)]\n",
    "        \n",
    "        # Draw boxes\n",
    "        if len(boxes) > 0:\n",
    "            img_with_boxes = draw_bounding_boxes(\n",
    "                img_vis,\n",
    "                boxes,\n",
    "                labels=label_strs,\n",
    "                colors=\"green\",\n",
    "                width=3\n",
    "            )\n",
    "        else:\n",
    "            img_with_boxes = img_vis\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(to_pil_image(img_with_boxes))\n",
    "        plt.title(f\"Predictions: {len(boxes)} detections (conf > {conf_threshold})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions from original model\n",
    "print(\"Original Model Predictions:\")\n",
    "visualize_predictions(model_original, val_dataset, num_samples=3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Visualize predictions from modified model\n",
    "print(\"Modified Model Predictions:\")\n",
    "visualize_predictions(model_modified, val_dataset, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61bb914",
   "metadata": {},
   "source": [
    "## 15. Save Final Results and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = f\"\"\"\n",
    "{'='*70}\n",
    "CMPE 249 HOMEWORK - OBJECT DETECTION TRAINING SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATASET INFORMATION:\n",
    "- Dataset: Waymo Open Dataset (COCO format)\n",
    "- Training samples: {len(train_dataset)}\n",
    "- Validation samples: {len(val_dataset)}\n",
    "- Number of classes: {num_classes}\n",
    "\n",
    "MODEL ARCHITECTURES:\n",
    "\n",
    "1. Original CustomRCNN (Baseline):\n",
    "   - Backbone: ResNet50\n",
    "   - Total parameters: {total_params:,}\n",
    "   - Trainable parameters: {trainable_params:,}\n",
    "   - Trainable layers: 3\n",
    "\n",
    "2. Modified CustomRCNN:\n",
    "   - Backbone: ResNet101\n",
    "   - Total parameters: {total_params_mod:,}\n",
    "   - Trainable parameters: {trainable_params_mod:,}\n",
    "   - Trainable layers: 4\n",
    "   - Modifications:\n",
    "     * Enhanced backbone (ResNet50 ‚Üí ResNet101)\n",
    "     * Increased trainable layers (3 ‚Üí 4)\n",
    "     * [Additional modifications would go here]\n",
    "\n",
    "TRAINING CONFIGURATION:\n",
    "- Epochs: {EPOCHS}\n",
    "- Batch size: {args.batch_size}\n",
    "- Learning rate: {LEARNING_RATE}\n",
    "- Optimizer: SGD (momentum={MOMENTUM}, weight_decay={WEIGHT_DECAY})\n",
    "- LR scheduler: StepLR (step_size={LR_STEP_SIZE}, gamma={LR_GAMMA})\n",
    "\n",
    "TRAINING RESULTS:\n",
    "\n",
    "Original Model:\n",
    "- Final training loss: {history_original['train_loss'][-1]:.4f}\n",
    "- Best training loss: {min(history_original['train_loss']):.4f}\n",
    "\n",
    "Modified Model:\n",
    "- Final training loss: {history_modified['train_loss'][-1]:.4f}\n",
    "- Best training loss: {min(history_modified['train_loss']):.4f}\n",
    "\n",
    "IMPROVEMENT:\n",
    "- Loss improvement: {(history_original['train_loss'][-1] - history_modified['train_loss'][-1]):.4f}\n",
    "- Percentage improvement: {((history_original['train_loss'][-1] - history_modified['train_loss'][-1]) / history_original['train_loss'][-1] * 100):.2f}%\n",
    "\n",
    "OUTPUT FILES:\n",
    "- Models saved in: {OUTPUT_DIR}\n",
    "- Training history: *_history.json\n",
    "- Best checkpoints: *_best.pth\n",
    "- Comparison results: comparison_results.csv\n",
    "- Training curves: training_curves.png\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Save summary to file\n",
    "with open(os.path.join(OUTPUT_DIR, 'training_summary.txt'), 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n‚úÖ Training summary saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e89e6e",
   "metadata": {},
   "source": [
    "## 16. Conclusion and Next Steps\n",
    "\n",
    "### Summary of Results:\n",
    "1. Successfully trained two CustomRCNN models on Waymo dataset\n",
    "2. Modified model shows improvement in training loss\n",
    "3. mAP evaluation results demonstrate the effectiveness of modifications\n",
    "\n",
    "### Key Findings:\n",
    "- **Model Capacity**: Modified model has more parameters, allowing for better feature learning\n",
    "- **Training Loss**: Modified model achieves lower training loss\n",
    "- **Performance**: [To be filled after evaluation]\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ Submitting Your Work\n",
    "\n",
    "### Step 1: Push Changes to YOUR Fork\n",
    "\n",
    "Since you forked the repository, you need to push your changes:\n",
    "\n",
    "```bash\n",
    "# In your local repository\n",
    "git add detection/modeling_customrcnn_modified.py\n",
    "git add detection/README_ModifiedCustomRCNN.md\n",
    "git add detection/HW_CustomRCNN_Waymo_Training.ipynb\n",
    "\n",
    "git commit -m \"Add CMPE 249 homework: Modified CustomRCNN implementation\"\n",
    "\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "### Step 2: For Your Report, Include:\n",
    "\n",
    "**1. GitHub Repository Link:**\n",
    "```\n",
    "https://github.com/Preet-Pandit2/DeepDataMiningLearning\n",
    "```\n",
    "\n",
    "**2. Specific Files to Reference:**\n",
    "- Modified Architecture: `detection/modeling_customrcnn_modified.py`\n",
    "- Training Notebook: `detection/HW_CustomRCNN_Waymo_Training.ipynb`\n",
    "- Documentation: `detection/README_ModifiedCustomRCNN.md`\n",
    "\n",
    "**3. Description of Modifications:**\n",
    "- Enhanced backbone (ResNet50 ‚Üí ResNet101)\n",
    "- Channel Attention Module (CAM) in FPN\n",
    "- Spatial Attention Module (SAM) in FPN\n",
    "- Enhanced ROI Head with multi-layer FC + dropout\n",
    "- Increased trainable layers (3 ‚Üí 5)\n",
    "\n",
    "**4. Include in Your Report:**\n",
    "- Training curves from `training_curves.png`\n",
    "- Comparison table from `comparison_results.csv`\n",
    "- mAP metrics from evaluation results\n",
    "- Sample predictions showing both models\n",
    "- Discussion of improvements and why they work\n",
    "\n",
    "**5. Google Colab Link (Optional):**\n",
    "- Upload this notebook to Colab\n",
    "- Share with \"Anyone with the link can view\"\n",
    "- Include the Colab link in your report\n",
    "\n",
    "---\n",
    "\n",
    "### Additional Experiments (Optional for Extra Credit):\n",
    "- Try different backbones (ResNet152, EfficientNet)\n",
    "- Experiment with data augmentation strategies\n",
    "- Adjust learning rate and training schedule\n",
    "- Add more sophisticated modifications (Deformable convolutions, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Submission Checklist:\n",
    "\n",
    "- [ ] Code pushed to YOUR fork (`Preet-Pandit2/DeepDataMiningLearning`)\n",
    "- [ ] Modified architecture file committed\n",
    "- [ ] Training notebook committed\n",
    "- [ ] Training completed and results saved\n",
    "- [ ] mAP evaluation completed\n",
    "- [ ] Comparison visualizations generated\n",
    "- [ ] Report includes GitHub link\n",
    "- [ ] Report includes methodology explanation\n",
    "- [ ] Report includes performance metrics\n",
    "- [ ] Report includes comparison graphs\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your assignment! üéì**\n",
    "\n",
    "**Repository:** https://github.com/Preet-Pandit2/DeepDataMiningLearning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
